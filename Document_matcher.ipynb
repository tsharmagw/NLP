{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@author - Tejasvi Sharma\n",
    "#import libraries\n",
    "import gensim\n",
    "import nltk\n",
    "import nltk.data\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to read file one by one, save data in a list\n",
    "\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "dir_base = \"./data/\"\n",
    "\n",
    "\n",
    "def read_file(filename):\n",
    "    input_file_text = open(filename , encoding='utf-8').read()\n",
    "    return input_file_text\n",
    "\n",
    "    \n",
    "def read_directory_files(directory):\n",
    "    file_texts = []\n",
    "    files = [f for f in listdir(directory) if isfile(join(directory, f))]\n",
    "    for f in files:\n",
    "        file_texts.append(read_file(join(directory, f) ))\n",
    "    return file_texts\n",
    "    \n",
    "text_corpus = read_directory_files(dir_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize data, tokenize whole document once\n",
    "token_corpus= [treebank_tokenizer.tokenize(document) for document in text_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stop words\n",
    "#use lemmatization to find root words, as some might use different forms of same word.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "pattern = re.compile(\"[A-Za-z./-]*\") #only takes words which have only letters, can also have acronyms, which can be a jargon\n",
    "#stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = set(stopwords.words('english')) \n",
    "corpus_filtered=[]\n",
    "for document in token_corpus: \n",
    "    document_corpus=[]\n",
    "    for word in document:\n",
    "        if(word not in stop_words):\n",
    "            #removing alphanumeric, so that alphanumeric wont be added in dictionary\n",
    "            if(pattern.fullmatch(word)):\n",
    "                document_corpus.append(lemmatizer.lemmatize(str.lower(word).replace(\".\", \"\"))) #removed . as many words had to forms. example example.\n",
    "    corpus_filtered.append(document_corpus)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as corpus has acronyms, check whether the acronyms are present in corpus or not\n",
    "#this step is required if corpus acronyms\n",
    "#corpus_filtered[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audit\n",
      "Number of words in dictionary: 3829\n"
     ]
    }
   ],
   "source": [
    "#create a dictionary from corpus\n",
    "dictionary = gensim.corpora.Dictionary(corpus_filtered)\n",
    "print(dictionary[5])\n",
    "print(\"Number of words in dictionary:\",len(dictionary))\n",
    "# for i in range(len(dictionary)):\n",
    "#     print(i, dictionary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting numbers of the word and the number of time it occurs in that document.(Term frequency)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in corpus_filtered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a tf-idf model from corpus\n",
    "tf_idf = gensim.models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = gensim.similarities.Similarity('/media/tejasvi-ts/My_Files/f18_ds_nlp/homework/homework_1/dir_sim/',tf_idf[corpus],\n",
    "                                      num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.06732434  0.04589299  0.54607368  0.59901953  0.63795596  0.01676748\n",
      "  0.078996    0.01244059  0.02219053  0.04291663  0.00741527  0.07085299\n",
      "  0.01845528  0.02686126  0.04910208  0.10107413  0.08841018  0.08548425\n",
      "  0.00860582  0.01216373  0.0143261   0.01580157  0.0264921   0.02096275\n",
      "  0.02392753  0.04421113  0.02416983  0.06441585  0.0298551   0.037147\n",
      "  0.01229415  0.01202513  0.03113357  0.00759161  0.02138072  0.01551744\n",
      "  0.05650915  0.03298256  0.04764617  0.02986821  0.04929609  0.0409489\n",
      "  0.00524068  0.06515984  0.25890231  0.05219238  0.05204027  0.02736784\n",
      "  0.09526758  0.26274782  0.00462185  0.01548065  0.04341775  0.06014821\n",
      "  0.01636594  0.0312711   0.01634337  0.0057677   0.03937663  0.00815066\n",
      "  0.04056709  0.07860883  0.02613876  0.02070126  0.02881328  0.02674234\n",
      "  0.03058406  0.02696064  0.03458256  0.01010247  0.0427427   0.00676735\n",
      "  0.01015245  0.03466513  0.06519137  0.08855504  0.0397654   0.0331906\n",
      "  0.0288224   0.00443691  0.01032401  0.02330595  0.03562814  0.02694421\n",
      "  0.05492827  0.00891466  0.04108572  0.01615604  0.11373643  0.03693835\n",
      "  0.01981657  0.06088959  0.09031197  0.03860265  0.01418184  0.0287468\n",
      "  0.11342871  0.18338434  0.05986405  0.02976105  0.5588513   0.02677715\n",
      "  0.06303804  0.09873313  0.04830866  0.02951106  0.03657539  0.02180067\n",
      "  0.05852894  0.0519333   0.00762204  0.02364338  0.01127657  0.01928753\n",
      "  0.01654989  0.03369021  0.03975492  0.02565579  0.04111477  0.01970123\n",
      "  0.02434082  0.00839172  0.05484438  0.02007372  0.04048707  0.04219376\n",
      "  0.12709811  0.02605415  0.05985098  0.18161362  0.00866758  0.01768552\n",
      "  0.06786546  0.00987194  0.10797084  0.03910716  0.05216242  0.03067763\n",
      "  0.03799187  0.02019267  0.0080055   0.02722775]\n"
     ]
    }
   ],
   "source": [
    "#getting similarity\n",
    "#for document in corpus_filtered:\n",
    "\n",
    "query_doc_bow = dictionary.doc2bow(corpus_filtered[2] + corpus_filtered[3]+corpus_filtered[4])\n",
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "print(sims[query_doc_tf_idf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
